import requests
from bs4 import BeautifulSoup
import sqlite3
import re
import os
import time
import math
from collections import namedtuple, defaultdict
from typing import List, Optional, NamedTuple
import shutil
import json
import csv  # æ–°å¢csvæ¨¡å—
from datetime import datetime, timedelta


import matplotlib
matplotlib.use('Agg')  # åœ¨ import plt å‰è®¾ç½®éäº¤äº’åç«¯


import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from matplotlib.font_manager import FontProperties
from PIL import Image





# é…ç½®å¸¸é‡
HTML_FILE = "page.html"
DB_FILE = "submissions_get.db"
BASE_URL = "https://codecin.com"
MAX_USERS = 3200
MAX_121_43_66_70 = 30 
MAX_codecin = 3200  
# BASE_URL = "http://121.43.66.70"
# MAX_USERS = 30
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
# SEARCH_USER = True  # æ˜¯å¦æœç´¢ç”¨æˆ·æäº¤è®°å½•
SEARCH_USER = False  # æ˜¯å¦æœç´¢ç”¨æˆ·æäº¤è®°å½•
SEARCH_PAGE = True  # æ˜¯å¦æœç´¢æäº¤è®°å½•é¡µé¢
# SEARCH_PAGE = False  # æ˜¯å¦æœç´¢æäº¤è®°å½•é¡µé¢
MAX_PAGE = 3
cookies_æµ‹è¯• = {
    "sid": "wAnFRAjNrZDZwYt9oS1Pez4byt9zGHfC",
    "sid.sig": "a5eOYbZ0GljDPGsBkhu_6eSEg2c"
}
cookies_å®‹æ° = { 
    "sid": "80jCkfCQdYV8yU5wXXdWvrtPJu3f06p3",
    "sid.sig": "xxuNrxBFv_4OnGlfHCTQmfZ2k28"
}

# å®šä¹‰ç»“æ„åŒ–æ•°æ®ç±»å‹
Submission = namedtuple('Submission', [
    'record_id', 'status', 'problem_code', 'problem_name', 
    'problem_url', 'submitter_name', 'submitter_id', 'time', 
    'memory', 'language', 'submit_time', 'school', 'result_url'
])




# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']  # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜





def fix_date_format(date_str):
    """å°†éæ ‡å‡†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºYYYY-MM-DDæ ¼å¼"""
    parts = re.split(r'[-/: ]', date_str)
    
    if len(parts) < 3:
        return date_str
    
    year = parts[0]
    month = parts[1].zfill(2)
    day = parts[2].zfill(2)
    
    return f"{year}-{month}-{day}"

def generate_submission_report(names, db_path='submissions.db', output_file='submission_report.json', csv_file='submission_data.csv'):
    """
    ç”ŸæˆæŒ‡å®šç”¨æˆ·æœ€è¿‘7å¤©çš„æäº¤ç»Ÿè®¡æŠ¥å‘Šï¼Œå¹¶å°†åŸå§‹æ•°æ®å†™å…¥CSV
    
    å‚æ•°:
    names (list): éœ€è¦ç»Ÿè®¡çš„ç”¨æˆ·ååˆ—è¡¨
    db_path (str): SQLiteæ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'submissions.db'
    output_file (str): è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'submission_report.json'
    csv_file (str): è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'submission_data.csv'
    """
    
    # 1. ç¡®å®šæ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘7å¤©ï¼‰
    today = datetime.now().date()
    dates = [today - timedelta(days=i) for i in range(7)]
    date_strs = [d.strftime("%Y-%m-%d") for d in dates]
    
    # å­˜å‚¨åŸå§‹æ•°æ®ç”¨äºCSV
    all_rows = []
    column_names = []
    
    # 2. è¿æ¥æ•°æ®åº“å¹¶æ‰§è¡ŒæŸ¥è¯¢
    try:
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        # è·å–åˆ—å
        c.execute("PRAGMA table_info(submissions)")
        column_info = c.fetchall()
        column_names = [col[1] for col in column_info]  # åˆ—ååœ¨å…ƒç»„çš„ç¬¬äºŒä¸ªä½ç½®
        
        # æ„å»ºSQLæŸ¥è¯¢
        placeholders = ', '.join(['?'] * len(names))
        query = f"""
        SELECT *
        FROM submissions
        WHERE submitter_name IN ({placeholders})
        """
        # AND language = 'Python 3'
        
        # æ‰§è¡ŒæŸ¥è¯¢
        c.execute(query, names)
        rows = c.fetchall()
        all_rows = rows  # ä¿å­˜æ‰€æœ‰åŸå§‹è¡Œæ•°æ®
        
    except sqlite3.Error as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
        return
    finally:
        if conn:
            conn.close()
    
    # 3. å¤„ç†æŸ¥è¯¢ç»“æœ - åœ¨Pythonä¸­å¤„ç†æ—¥æœŸ
    user_stats = defaultdict(lambda: defaultdict(int))
    filtered_rows = []  # ç”¨äºå­˜å‚¨ç¬¦åˆæ—¥æœŸæ¡ä»¶çš„è¡Œ
    
    # å…ˆè·å–åˆ—ç´¢å¼•
    name_index = column_names.index('submitter_name')
    time_index = column_names.index('submit_time')
    
    for row in rows:
        submitter_name = row[name_index]
        submit_time = row[time_index]
        
        try:
            # æå–æ—¥æœŸéƒ¨åˆ†å¹¶ä¿®å¤æ ¼å¼
            raw_date = submit_time.split()[0]  # å–ç©ºæ ¼å‰çš„éƒ¨åˆ†ä½œä¸ºæ—¥æœŸ
            fixed_date = fix_date_format(raw_date)
            
            # è½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡
            submit_date = datetime.strptime(fixed_date, "%Y-%m-%d").date()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æœ€è¿‘7å¤©å†…
            if submit_date in dates:
                user_stats[submitter_name][submit_date.strftime("%Y-%m-%d")] += 1
                # ä¿å­˜è¯¥è¡Œæ•°æ®ï¼ˆåŒ…æ‹¬åŸå§‹æ—¥æœŸæ ¼å¼ï¼‰
                filtered_rows.append(row)
                
        except Exception as e:
            print(f"å¤„ç†æ—¥æœŸæ—¶å‡ºé”™: {submit_time} -> {e}")
    
    # 4. æ„å»ºç»“æœå­—å…¸
    result = {
        "times": date_strs,
        "datas": {}
    }
    
    for name in names:
        daily_counts = [user_stats[name].get(d, 0) for d in date_strs]
        result["datas"][name] = daily_counts
    
    # 5. å†™å…¥JSONæ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆåˆ°: {output_file}")
        
        # è°ƒè¯•è¾“å‡º
        print("æ—¥æœŸèŒƒå›´:", date_strs)
        print("ç»Ÿè®¡ç»“æœç¤ºä¾‹:")
        for name, counts in result["datas"].items():
            print(f"{name}: {counts}")
            
    except IOError as e:
        print(f"æ–‡ä»¶å†™å…¥é”™è¯¯: {e}")

    
    columns_to_exclude = ['result_url', 'problem_url', 'record_id']
    filtered_rows = [list(row) for row in filtered_rows if all(col not in columns_to_exclude for col in row)]
    
    
    # 6. å†™å…¥CSVæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰åŸå§‹åˆ—ï¼‰
    if filtered_rows:
        try:
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                # å†™å…¥åˆ—æ ‡é¢˜
                writer.writerow(column_names)
                # å†™å…¥æ•°æ®è¡Œ
                writer.writerows(filtered_rows)
            print(f"CSVæ•°æ®å·²å†™å…¥: {csv_file}")
        except Exception as e:
            print(f"å†™å…¥CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
    else:
        print("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®å¯å†™å…¥CSV")





# ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
def load_data_from_json(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

# ä¸»ç»˜å›¾å‡½æ•°
def plot_line_chart(data):
    # å‡†å¤‡æ—¶é—´æ•°æ®ï¼ˆè½¬æ¢ä¸ºdatetimeå¯¹è±¡ï¼‰
    dates = [datetime.strptime(t, "%Y-%m-%d") for t in data["times"]]
    
    # è®¾ç½®å›¾å½¢
    plt.figure(figsize=(14, 8), dpi=100)
    plt.title("æœªæ¥å¼•æ“-åˆ·é¢˜å˜åŒ–è¶‹åŠ¿", fontsize=16, fontweight='bold')
    plt.xlabel("æ—¥æœŸ", fontsize=14)
    plt.ylabel("æäº¤æ¬¡æ•°", fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.5)
    
    # ä¸ºæ¯ä¸ªäººç»˜åˆ¶æŠ˜çº¿
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'X']
    colors = plt.cm.tab10(np.linspace(0, 1, len(data["datas"])))
    
    for i, (name, values) in enumerate(data["datas"].items()):
        plt.plot(
            dates, 
            values,
            linewidth=2.5,
            marker=markers[i % len(markers)],
            markersize=8,
            color=colors[i],
            alpha=0.9,
            label=name
        )
    
    # è®¾ç½®æ—¥æœŸæ ¼å¼
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))
    plt.gcf().autofmt_xdate()
    
    # æ·»åŠ å›¾ä¾‹
    plt.legend(title="äººå‘˜åˆ—è¡¨", fontsize=11, title_fontsize=12, loc='best')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plt.savefig('static/user.png', dpi=150)
    
    # plt.show()





def remove_white_background(input_path, output_path, tolerance=15):
    """
    å»é™¤å›¾ç‰‡ä¸­çš„ç™½è‰²èƒŒæ™¯ï¼ˆè®¾ä¸ºé€æ˜ï¼‰
    
    å‚æ•°:
    input_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
    output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼ˆæ”¯æŒPNGæ ¼å¼ï¼‰
    tolerance: é¢œè‰²å®¹å·®ï¼ˆ0-255ï¼‰ï¼Œå€¼è¶Šå¤§è¢«è§†ä¸ºç™½è‰²çš„é¢œè‰²èŒƒå›´è¶Šå¹¿
    """
    img = Image.open(input_path).convert("RGBA")
    datas = img.getdata()
    
    new_data = []
    for item in datas:
        # æ£€æµ‹æ˜¯å¦ä¸ºç™½è‰²åƒç´ ï¼ˆæˆ–æ¥è¿‘ç™½è‰²ï¼‰
        if item[0] > 255 - tolerance and item[1] > 255 - tolerance and item[2] > 255 - tolerance:
            # è®¾ä¸ºå®Œå…¨é€æ˜ (R, G, B, A) å…¶ä¸­ A=0 è¡¨ç¤ºé€æ˜
            new_data.append((255, 255, 255, 0))
        else:
            # ä¿ç•™åŸå§‹é¢œè‰²å’Œé€æ˜åº¦
            new_data.append(item)
    
    img.putdata(new_data)
    img.save(output_path, "PNG")
    print(f"ç™½è‰²èƒŒæ™¯å·²ç§»é™¤ï¼ä¸´æ—¶æ–‡ä»¶ä¿å­˜è‡³: {output_path}")

def add_scaled_watermark(background_path, watermark_path, output_path, position=(0, 0), opacity=1.0, tolerance=15, scale=0.2):
    """
    å°†æ°´å°å›¾ç‰‡å®½åº¦ç¼©æ”¾åˆ°èƒŒæ™¯å›¾çš„æŒ‡å®šæ¯”ä¾‹åæ·»åŠ åˆ°ä¸»å›¾ä¸Š
    
    å‚æ•°:
    background_path: èƒŒæ™¯å›¾ç‰‡è·¯å¾„
    watermark_path: æ°´å°å›¾ç‰‡è·¯å¾„
    output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
    position: æ°´å°ä½ç½® (x, y)ï¼Œé»˜è®¤ä¸ºå·¦ä¸Šè§’ (0, 0)
    opacity: æ°´å°é€æ˜åº¦ (0.0 - 1.0)
    tolerance: ç™½è‰²æ£€æµ‹å®¹å·®
    scale: æ°´å°å®½åº¦ç›¸å¯¹äºèƒŒæ™¯å®½åº¦çš„æ¯”ä¾‹ (ä¾‹å¦‚0.2è¡¨ç¤ºæ°´å°å®½åº¦=èƒŒæ™¯å®½åº¦Ã—0.2)
    """
    # 1. é¦–å…ˆå»é™¤æ°´å°å›¾çš„ç™½è‰²èƒŒæ™¯
    transparent_watermark_path = "transparent_temp.png"
    remove_white_background(watermark_path, transparent_watermark_path, tolerance)
    
    # 2. å‡†å¤‡èƒŒæ™¯å’Œæ°´å°å›¾ç‰‡
    background = Image.open(background_path).convert("RGBA")
    watermark = Image.open(transparent_watermark_path).convert("RGBA")
    
    # 3. è®¡ç®—æ–°çš„æ°´å°å°ºå¯¸ - å®½åº¦ä¸ºèƒŒæ™¯å®½åº¦çš„äº”åˆ†ä¹‹ä¸€
    new_width = int(background.width * scale)
    
    # è®¡ç®—æ–°é«˜åº¦ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
    aspect_ratio = watermark.height / watermark.width
    new_height = int(new_width * aspect_ratio)
    
    # 4. è°ƒæ•´æ°´å°å¤§å°ï¼ˆä½¿ç”¨é«˜è´¨é‡çš„é‡é‡‡æ ·ç®—æ³•ï¼‰
    watermark = watermark.resize((new_width, new_height), Image.LANCZOS)
    
    # 5. è°ƒæ•´æ°´å°é€æ˜åº¦
    if opacity < 1.0:
        alpha = watermark.getchannel("A")
        alpha = alpha.point(lambda p: p * opacity)
        watermark.putalpha(alpha)
    
    # 6. åˆ›å»ºé€æ˜å›¾å±‚æ”¾ç½®æ°´å°
    watermark_layer = Image.new("RGBA", background.size, (0, 0, 0, 0))
    watermark_layer.paste(watermark, position, watermark)
    
    # 7. åˆå¹¶å›¾ç‰‡
    combined = Image.alpha_composite(background, watermark_layer)
    combined.convert("RGB").save(output_path)
    print(f"æ°´å°æ·»åŠ å®Œæˆï¼å›¾ç‰‡å·²ä¿å­˜è‡³: {output_path}")
    print(f"æ°´å°å°ºå¯¸: {watermark.size} (ç¼©æ”¾æ¯”ä¾‹: {scale} = 1/{int(1/scale)})")
    print(f"æ°´å°ä½ç½®: ({position[0]}, {position[1]}) - å·¦ä¸Šè§’")




def merge_images_vertically(image_path1, image_path2, output_path):
    """
    å°†ä¸¤å¼ å›¾ç‰‡ç­‰æ¯”ç¼©æ”¾è‡³ç›¸åŒå®½åº¦åä¸Šä¸‹æ‹¼æ¥
    
    å‚æ•°:
    image_path1 -- ä¸Šæ–¹å›¾ç‰‡è·¯å¾„
    image_path2 -- ä¸‹æ–¹å›¾ç‰‡è·¯å¾„
    output_path -- ä¿å­˜è·¯å¾„
    """
    # æ‰“å¼€ä¸¤å¼ å›¾ç‰‡
    img1 = Image.open(image_path1)
    img2 = Image.open(image_path2)
    
    # é€‰æ‹©æœ€å¤§å®½åº¦ä½œä¸ºç›®æ ‡å®½åº¦
    target_width = max(img1.width, img2.width)
    
    # è®¡ç®—ç­‰æ¯”ç¼©æ”¾åçš„é«˜åº¦
    img1_ratio = img1.height / img1.width
    img2_ratio = img2.height / img2.width
    
    img1_height = int(target_width * img1_ratio)
    img2_height = int(target_width * img2_ratio)
    
    # åº”ç”¨ç›¸åŒçš„ç›®æ ‡å®½åº¦è¿›è¡Œç­‰æ¯”ç¼©æ”¾
    img1_resized = img1.resize((target_width, img1_height), Image.Resampling.LANCZOS)
    img2_resized = img2.resize((target_width, img2_height), Image.Resampling.LANCZOS)
    
    # åˆ›å»ºæ–°ç”»å¸ƒï¼ˆå®½åº¦ç›¸åŒï¼Œé«˜åº¦ä¸ºä¸¤å›¾ä¹‹å’Œï¼‰
    merged_height = img1_height + img2_height
    merged = Image.new('RGB', (target_width, merged_height), (255, 255, 255))
    
    # æ‹¼æ¥å›¾ç‰‡
    merged.paste(img1_resized, (0, 0))
    merged.paste(img2_resized, (0, img1_height))
    
    # ä¿å­˜ç»“æœ
    merged.save(output_path)








def fetch_page(url: str, output_file: str = HTML_FILE) -> bool:
    """
    æŠ“å–ç½‘é¡µå¹¶ä¿å­˜ä¸ºHTMLæ–‡ä»¶
    
    Args:
        url: è¦æŠ“å–çš„URL
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    headers = {'User-Agent': USER_AGENT}
    try:
        if "codecin" in url:
            cookie = cookies_å®‹æ°
        else:
            cookie = cookies_æµ‹è¯•
        response = requests.get(url, headers=headers, timeout=15, cookies=cookie)
        
        response.raise_for_status()
        
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(response.text)
            
        print(f"âœ… ç½‘é¡µå·²ä¿å­˜è‡³: {output_file}")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return False
    except IOError as e:
        print(f"âŒ æ–‡ä»¶å†™å…¥é”™è¯¯: {e}")
        return False

def parse_submission_row(row, row_index: int) -> Optional[Submission]:
    """
    è§£æå•è¡Œæäº¤è®°å½•
    
    Args:
        row: BeautifulSoupè¡¨æ ¼è¡Œå¯¹è±¡
        row_index: è¡Œç´¢å¼•(ç”¨äºé”™è¯¯æŠ¥å‘Š)
        
    Returns:
        ç»“æ„åŒ–æäº¤è®°å½•æˆ–None(è§£æå¤±è´¥æ—¶)
    """
    try:
        # æå–è®°å½•ID
        record_id = row.get('data-rid', '').strip()
        if not record_id:
            print(f"âš ï¸ ç¬¬{row_index+1}è¡Œç¼ºå°‘record_idï¼Œè·³è¿‡è¯¥è¡Œ")
            return None
        
        # çŠ¶æ€ä¿¡æ¯
        status_div = row.find('td', class_='col--status')
        status_text = ' '.join(status_div.get_text(strip=True).split()) if status_div else ""
        
        # é¢˜ç›®ä¿¡æ¯
        problem_td = row.find('td', class_='col--problem')
        problem_link = problem_td.find('a') if problem_td else None
        
        problem_code, problem_name, problem_url = "", "", ""
        if problem_link:
            b_tag = problem_link.find('b')
            problem_code = b_tag.text.strip() if b_tag else ""
            problem_name = problem_link.text.replace(problem_code, '', 1).strip()
            problem_url = f"{BASE_URL}{problem_link['href']}" if problem_link.get('href') else ""
        else:
            problem_name = problem_td.get_text(strip=True) if problem_td else ""
        
        # é€’äº¤è€…ä¿¡æ¯
        submitter_td = row.find('td', class_='col--submit-by')
        submitter_link = submitter_td.find('a') if submitter_td else None
        submitter_name = submitter_link.text.strip() if submitter_link else ""
        submitter_id = submitter_link['href'].split('/')[-1] if submitter_link and 'href' in submitter_link.attrs else ""
        
        # æå–æ€§èƒ½ä¿¡æ¯
        time = safe_extract_text(row, 'col--time')
        memory = safe_extract_text(row, 'col--memory')
        language = safe_extract_text(row, 'col--lang')
        
        # é€’äº¤æ—¶é—´
        submit_time_span = row.find('td', class_='col--submit-at')
        submit_time_span = submit_time_span.find('span') if submit_time_span else None
        submit_time = re.sub(r'\s+', ' ', submit_time_span.text.strip()) if submit_time_span else ""
        
        # å­¦æ ¡ä¿¡æ¯
        school = safe_extract_text(row, 'col--school')
        
        # ç»“æœé“¾æ¥
        result_url = f"{BASE_URL}/record/{record_id}"
        
        # åˆ›å»ºç»“æ„åŒ–å¯¹è±¡
        return Submission(
            record_id=record_id,
            status=status_text,
            problem_code=problem_code,
            problem_name=problem_name,
            problem_url=problem_url,
            submitter_name=submitter_name,
            submitter_id=submitter_id,
            time=time,
            memory=memory,
            language=language,
            submit_time=submit_time,
            school=school,
            result_url=result_url
        )
        
    except Exception as e:
        print(f"âŒ è§£æç¬¬{row_index+1}è¡Œæ—¶å‡ºé”™: {e}")
        return None

def safe_extract_text(row, class_name: str) -> str:
    """å®‰å…¨æå–æ–‡æœ¬å†…å®¹"""
    element = row.find('td', class_=class_name)
    return element.text.strip() if element else ""

def parse_submissions(input_file: str = HTML_FILE) -> List[Submission]:
    """
    ä»HTMLæ–‡ä»¶ä¸­è§£ææäº¤è®°å½•
    
    Args:
        input_file: HTMLæ–‡ä»¶è·¯å¾„
        
    Returns:
        æäº¤è®°å½•åˆ—è¡¨
    """
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        submissions = []
        
        submission_table = soup.find('table', class_='record_main__table')
        if not submission_table:
            print("âš ï¸ æœªæ‰¾åˆ°æäº¤è®°å½•è¡¨æ ¼")
            return submissions
        
        tbody = submission_table.find('tbody')
        if not tbody:
            print("âš ï¸ è¡¨æ ¼ä¸­æœªæ‰¾åˆ°tbodyéƒ¨åˆ†")
            return submissions
        
        rows = tbody.find_all('tr')
        if not rows:
            print("â„¹ï¸ è¡¨æ ¼ä¸­æ²¡æœ‰æ•°æ®è¡Œ")
            return submissions
        
        for i, row in enumerate(rows):
            submission = parse_submission_row(row, i)
            if submission:
                submissions.append(submission)
        
        print(f"âœ… æˆåŠŸè§£æ {len(submissions)} æ¡æäº¤è®°å½•")
        return submissions
        
    except Exception as e:
        print(f"âŒ è§£æHTMLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def setup_database() -> bool:
    """
    åˆå§‹åŒ–æ•°æ®åº“
    
    Returns:
        Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        
        # åˆ›å»ºæäº¤è®°å½•è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        c.execute('''CREATE TABLE IF NOT EXISTS submissions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    record_id TEXT NOT NULL UNIQUE,
                    status TEXT,
                    problem_code TEXT,
                    problem_name TEXT,
                    problem_url TEXT,
                    submitter_name TEXT,
                    submitter_id TEXT,
                    time TEXT,
                    memory TEXT,
                    language TEXT,
                    submit_time TEXT,
                    school TEXT,
                    result_url TEXT
                )''')
        
        conn.commit()
        conn.close()
        print(f"âœ… æ•°æ®åº“å·²å‡†å¤‡å°±ç»ª: {DB_FILE}")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def save_to_database(submissions: List[Submission]) -> int:
    """
    ä¿å­˜æäº¤è®°å½•åˆ°æ•°æ®åº“
    
    Args:
        submissions: æäº¤è®°å½•åˆ—è¡¨
        
    Returns:
        æˆåŠŸä¿å­˜çš„è®°å½•æ•°
    """
    if not submissions:
        return 0
        
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        
        saved_count = 0
        for sub in submissions:
            try:
                c.execute('''INSERT OR IGNORE INTO submissions (
                            record_id, status, problem_code, problem_name, problem_url,
                            submitter_name, submitter_id, time, memory, language,
                            submit_time, school, result_url
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                        tuple(sub))
                saved_count += 1
            except sqlite3.IntegrityError:
                # å¿½ç•¥é‡å¤è®°å½•
                pass
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜è®°å½• {sub.record_id} æ—¶å‡ºé”™: {e}")
        
        conn.commit()
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡è®°å½•åˆ°æ•°æ®åº“")
        return saved_count
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
        return 0
    finally:
        if 'conn' in locals():
            conn.close()

def process_page(url) -> int:
    """
    å¤„ç†å•ä¸ªé¡µé¢
    
    Args:
        url: é¡µé¢URL

    Returns:
        å¤„ç†æˆåŠŸçš„è®°å½•æ•°
    """
    
    if not fetch_page(url):
        return 0
    
    submissions = parse_submissions()
    if not submissions:
        return 0
    
    return save_to_database(submissions)

def estimate_remaining_time(start_time, processed, total):
    """ä¼°ç®—å‰©ä½™æ—¶é—´"""
    if processed == 0:
        return "æœªçŸ¥"
    
    elapsed = time.time() - start_time
    time_per_item = elapsed / processed
    remaining = (total - processed) * time_per_item
    
    if remaining > 3600:
        return f"{remaining/3600:.1f}å°æ—¶"
    elif remaining > 60:
        return f"{remaining/60:.1f}åˆ†é’Ÿ"
    return f"{remaining:.0f}ç§’"

def get_total_datas():
    """ä¸»ç¨‹åºé€»è¾‘"""
    if not setup_database():
        return
    
    print("\nğŸš€ å¼€å§‹æŠ“å–æäº¤è®°å½•...")
    total_users = MAX_USERS
    start_time = time.time()
    processed_users = 0

    if SEARCH_PAGE:
        page = 1
        while True and page <= MAX_PAGE:
            url = f"{BASE_URL}/record?page={page}"
            print(f"\n{'='*50}")
            print(f"ğŸ“‘ ç¬¬ {page} é¡µ | ğŸŒ {url}")
            print(f"{'='*50}")

            saved_count = process_page(url)
            if saved_count == 0:
                break
            page += 1
            # exit()
            # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(0.5)
    
    if SEARCH_USER :
        for user_id in range(total_users):
            page = 1
            user_records = 0
            
            # å¤„ç†å½“å‰ç”¨æˆ·çš„æ‰€æœ‰é¡µé¢
            while True:
                url = f"{BASE_URL}/record?uidOrName={user_id}&page={page}"
                print(f"\n{'='*50}")
                print(f"ğŸ‘¤ ç”¨æˆ· {user_id} | ğŸ“‘ ç¬¬ {page} é¡µ | ğŸŒ {url}")
                print(f"{'='*50}")

                saved_count = process_page(url)
                if saved_count == 0:
                    break
                user_records += saved_count
                page += 1
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                # time.sleep(0.5)
            
            processed_users += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if user_id % 10 == 0 or user_id == total_users - 1:
                elapsed = time.time() - start_time
                remaining_time = estimate_remaining_time(
                    start_time, processed_users, total_users
                )
                print(f"\nğŸ“Š è¿›åº¦: {processed_users}/{total_users} ç”¨æˆ· ({processed_users/total_users*100:.1f}%)")
                print(f"â±ï¸ ç”¨æ—¶: {elapsed:.1f}ç§’ | å‰©ä½™: {remaining_time}")
    
    print("\nğŸ‰ æ‰€æœ‰æ“ä½œå·²å®Œæˆï¼")
    print(f"ğŸ“‚ æ•°æ®åº“ä½ç½®: {DB_FILE}")


def o1ne_get_codecin_data_output():

    print("å¼€å§‹æŠ“å–æäº¤è®°å½•...")

    start_time = time.time()
    MAX_USERS = MAX_121_43_66_70
    BASE_URL = "http://121.43.66.70"
    get_total_datas()
    MAX_USERS = MAX_codecin
    BASE_URL = "https://codecin.com"
    get_total_datas()
    
    end_time = time.time()

    print(f"âœ… æŠ“å–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)


def t2wo_remove_struct_name():

    print("å¼€å§‹åˆ é™¤æ— æ„ä¹‰çš„æäº¤è®°å½•...")
    
    start_time = time.time()

    # è¿æ¥åˆ°ä½ çš„SQLiteæ•°æ®åº“ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“è·¯å¾„ï¼‰
    db_path = 'submissions_get.db'  # ä¿®æ”¹ä¸ºå®é™…çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„

    # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
    if os.path.exists('submissions.db'):  # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        os.remove('submissions.db')
    shutil.copyfile(db_path, 'submissions.db')

    db_path = 'submissions.db'

    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æ‰§è¡Œæ›´æ–°æ“ä½œ
        cursor.execute(
            "UPDATE submissions SET submitter_name = ? WHERE submitter_name = ?",
            ('ææ³°ç„¶', 'ææ³°ç„¶2014')
        )
        update_count = cursor.rowcount
        
        # æ‰§è¡Œåˆ é™¤æ“ä½œï¼šåˆ é™¤problem_nameä¸º*çš„è®°å½•
        cursor.execute(
            "DELETE FROM submissions WHERE problem_name = '*'"
        )
        delete_count = cursor.rowcount
        
        # æäº¤äº‹åŠ¡
        conn.commit()

        print(f"æˆåŠŸæ›´æ–° {update_count} æ¡è®°å½•")
        print(f"æˆåŠŸåˆ é™¤ {delete_count} æ¡è®°å½•")
        
    except sqlite3.Error as e:
        print("æ•°æ®åº“æ“ä½œå‡ºé”™:", e)
        conn.rollback()
        
    finally:
        # å…³é—­è¿æ¥
        if conn:
            conn.close()

    end_time = time.time()
    print(f"âœ… åˆ é™¤å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)


def t3hree_count_user_result():

    print("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

    start_time = time.time()

    #user_names = ["èƒ¡æ–¯æ¡", "é‚±å­å¢¨", "å‘¨æ·¼", "å‘¨å ƒç¦¾"]
    user_names = ["è¡£é‚¢æ¡‰è‹’", "ç« æ–°èƒœ", "é«˜äº‘é“®", "ç‹å­å¢¨", "å’¸æ™¯ç†™", "ææ³°ç„¶", "åˆ˜å“å¥•", "æ®·æ¢“éª", "é‚±å­å¢¨"]
    generate_submission_report(names=user_names)

    end_time = time.time()
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)




def t4four_count_ok_question_num():

    print("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

    start_time = time.time()

    
    # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
    DB_PATH = 'submissions.db'  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“è·¯å¾„
    JSON_FILE = 'statistics.json'  # è¾“å‡ºJSONæ–‡ä»¶å

    # ç›®æ ‡å§“ååˆ—è¡¨
    target_names = ["è¡£é‚¢æ¡‰è‹’", "ç« æ–°èƒœ", "é«˜äº‘é“®", "ç‹å­å¢¨", "å’¸æ™¯ç†™", 
                "ææ³°ç„¶", "åˆ˜å“å¥•", "å¾å…ˆè“¬", "æ®·æ¢“éª", "é‚±å­å¢¨"]

    # target_names = ["èƒ¡æ–¯æ¡", "é‚±å­å¢¨", "å‘¨æ·¼", "å‘¨å ƒç¦¾"]
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # ä¸ºæ¯ä¸ªå§“åå•ç‹¬æŸ¥è¯¢å¹¶ç»Ÿè®¡
    result_list = []

    for name in target_names:
        # ä¸ºæ¯ä¸ªå§“åå•ç‹¬æ„å»ºæŸ¥è¯¢
        query = """
        SELECT COUNT(DISTINCT problem_code)
        FROM submissions 
        WHERE submitter_name = ?
        AND status LIKE '%Accepted%'
        """
        #   AND language = "Python 3" 
        
        # æ‰§è¡ŒæŸ¥è¯¢
        cursor.execute(query, (name,))
        count = cursor.fetchone()[0]  # è·å–è®¡æ•°ç»“æœ

        print(f"{name} çš„å®Œæˆé¢˜ç›®æ•°é‡ä¸ºï¼š{count}")
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        result_list.append({
            "name": name,
            "num": count
        })

    # å…³é—­æ•°æ®åº“è¿æ¥
    conn.close()

    # æ„å»ºè¾“å‡ºæ•°æ®ç»“æ„
    output = {"datas": result_list}

    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=4)

    print(f"ç»Ÿè®¡å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {JSON_FILE}")

    end_time = time.time()
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)



def f5ive_make_user_png():

    print("å¼€å§‹ç”Ÿæˆç”¨æˆ·æŠ˜çº¿å›¾æŠ¥å‘Š...")

    start_time = time.time()


    # ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
    json_filename = "submission_report.json"  # æ›¿æ¢ä¸ºä½ çš„JSONæ–‡ä»¶å
    data = load_data_from_json(json_filename)
    
    # æ‰“å°æ–‡ä»¶è·¯å¾„ç¡®è®¤
    print(f"ä»æ–‡ä»¶ '{json_filename}' åŠ è½½æ•°æ®...")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½æˆåŠŸ
    if data:
        print(f"æˆåŠŸåŠ è½½æ•°æ®: {len(data['datas'])} ä¸ªäººçš„æ•°æ®")
        print(f"æ—¶é—´èŒƒå›´: {data['times'][0]} è‡³ {data['times'][-1]}")
        
        # ç»˜åˆ¶æŠ˜çº¿å›¾
        plot_line_chart(data)
        print("å›¾è¡¨å·²ç”Ÿæˆå¹¶ä¿å­˜ä¸º 'static/user.png'")
    else:
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå†…å®¹")

    
    end_time = time.time()
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)



def s6ix_make_question_png():

    print("å¼€å§‹ç”Ÿæˆæ­£ç¡®é¢˜ç›®å›¾æŠ¥å‘Š...")
    start_time = time.time()

    
    # ä»æ–‡ä»¶è¯»å–JSONæ•°æ®
    with open('statistics.json', 'r', encoding='utf-8') as f:
        json_data = f.read()

    # è§£æ JSON æ•°æ®
    data = json.loads(json_data)
    datas = data['datas']

    # æŒ‰æ•°é‡ä»å°åˆ°å¤§æ’åº
    datas_sorted = sorted(datas, key=lambda x: x['num'])

    # æå–æ’åºåçš„å§“åå’Œæ•°é‡
    names = [item['name'] for item in datas_sorted]
    values = [item['num'] for item in datas_sorted]

    # è®¾ç½®å›¾å½¢å¤§å°å’ŒDPI
    plt.figure(figsize=(10, 6), dpi=100)
    # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜ï¼‰
    plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    # åˆ›å»ºæ¡å½¢å›¾ï¼ˆæŒ‰å‡åºæ’åˆ—ï¼‰
    bars = plt.bar(names, values, color=['#4C72B0', '#55A868', '#DD8452'])

    # æ·»åŠ æ•°æ®æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # å‚ç›´åç§»
                    textcoords="offset points",
                    ha='center', va='bottom',
                    fontsize=10)

    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title('æœªæ¥å¼•æ“ å­¦å‘˜å·²é€šè¿‡é¢˜ç›®æ•°é‡', fontsize=14, pad=20)
    plt.xlabel('å§“å', fontsize=12)
    plt.ylabel('å·²é€šè¿‡çš„é¢˜ç›®æ•°é‡', fontsize=12)

    # è®¾ç½®ç½‘æ ¼çº¿
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # æ·»åŠ èƒŒæ™¯è‰²
    plt.gca().set_facecolor('#F5F5F5')

    # æ—‹è½¬Xè½´æ ‡ç­¾é˜²æ­¢é‡å ï¼ˆç‰¹åˆ«æ˜¯å½“åå­—è¾ƒé•¿æ—¶ï¼‰
    plt.xticks(rotation=20)

    # è°ƒæ•´è¾¹è·
    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    plt.savefig('static/question.png', dpi=150)

    # æ˜¾ç¤ºå›¾å½¢
    # plt.show()

    print("âœ… çŠ¶æ€å›¾è¡¨å·²ä¿å­˜ä¸º static/question.png")

    end_time = time.time()
    print(f"âœ… ç»Ÿè®¡å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

    time.sleep(1)
    



def s7even_add_watermark():

    print("å¼€å§‹æ·»åŠ æ°´å°...")

    start_time = time.time()

    # ä¸»å›¾ç‰‡è·¯å¾„ï¼ˆèƒŒæ™¯ï¼‰
    background_image = "static/user.png"
    # æ°´å°å›¾ç‰‡è·¯å¾„
    watermark_image = "static/logo.png"
    # è¾“å‡ºå›¾ç‰‡è·¯å¾„
    output_image = "static/user.png"
    
    # åœ¨å·¦ä¸Šè§’æ·»åŠ å»é™¤ç™½åº•çš„æ°´å°ï¼Œå®½åº¦ä¸ºèƒŒæ™¯å®½åº¦çš„1/5
    add_scaled_watermark(
        background_image, 
        watermark_image, 
        output_image,
        position=(0, 0),  # å·¦ä¸Šè§’ä½ç½®
        opacity=0.7,       # 70%é€æ˜åº¦
        tolerance=15,      # é¢œè‰²å®¹å·®
        scale=0.2          # æ°´å°å®½åº¦=èƒŒæ™¯å®½åº¦Ã—0.2 (å³1/5)
    )
    
    # ä¸»å›¾ç‰‡è·¯å¾„ï¼ˆèƒŒæ™¯ï¼‰
    background_image = "static/question.png"
    # è¾“å‡ºå›¾ç‰‡è·¯å¾„
    output_image = "static/question.png"
    # åœ¨å·¦ä¸Šè§’æ·»åŠ å»é™¤ç™½åº•çš„æ°´å°ï¼Œå®½åº¦ä¸ºèƒŒæ™¯å®½åº¦çš„1/5
    add_scaled_watermark(
        background_image, 
        watermark_image, 
        output_image,
        position=(0, 0),  # å·¦ä¸Šè§’ä½ç½®
        opacity=0.7,       # 70%é€æ˜åº¦
        tolerance=15,      # é¢œè‰²å®¹å·®
        scale=0.2          # æ°´å°å®½åº¦=èƒŒæ™¯å®½åº¦Ã—0.2 (å³1/5)
    )


    end_time = time.time()
    print(f"âœ… æ·»åŠ æ°´å°å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    time.sleep(1)
        


def e8ight_stick_png():

    print("å¼€å§‹åˆå¹¶å›¾ç‰‡...")

    start_time = time.time()


    # ä½¿ç”¨ç¤ºä¾‹
    merge_images_vertically("static/user.png", "static/question.png", "static/report.png")

    end_time = time.time()
    print(f"âœ… åˆå¹¶å›¾ç‰‡å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
    time.sleep(1)






def main():
    

    o1ne_get_codecin_data_output()

    t2wo_remove_struct_name()

    t3hree_count_user_result()

    t4four_count_ok_question_num()

    f5ive_make_user_png()

    s6ix_make_question_png()

    s7even_add_watermark()

    e8ight_stick_png()

    

    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    os.remove("page.html")
    os.remove("statistics.json")
    os.remove("submission_report.json")
    os.remove("submissions.db")
    os.remove("transparent_temp.png")

if __name__ == "__main__":
    last_day = time.localtime().tm_mday


    while True:

        # è·å–å½“å‰åˆ†é’Ÿ
        day = time.localtime().tm_mday
        month = time.localtime().tm_mon
        year = time.localtime().tm_year
        hour = time.localtime().tm_hour
        minute = time.localtime().tm_min
        print(f"[æ£€æŸ¥æ—¶é—´] å½“å‰æ—¶é—´: {year}/{month}/{day} {hour}:{minute}")


        if day != last_day:

            SEARCH_USER = True
            SEARCH_PAGE = False

            print(f"[æ£€æŸ¥æ—¥æœŸ] å½“å‰æ—¥æœŸ: {year}/{month}/{day}")
            
            print("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
            s = time.time()

            main()

            print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time.time() - s:.2f}ç§’")

            last_day = day
            SEARCH_PAGE = True
            SEARCH_USER = False

        elif hour % 5 == 0:

            MAX_PAGE = 20

            print(f"[æ£€æŸ¥å°æ—¶æ•°] å½“å‰å°æ—¶: {hour}")

            print("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")

            s = time.time()

            main()

            print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time.time() - s:.2f}ç§’")

            MAX_PAGE = 1



        elif minute % 5 == 0:

            print(f"[æ£€æŸ¥åˆ†é’Ÿæ•°] å½“å‰åˆ†é’Ÿ: {minute}")
            print("å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
            s = time.time()

            main()

            print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time.time() - s:.2f}ç§’")


        else:
            print(f"[è·³è¿‡] å½“å‰åˆ†é’Ÿ {minute} ä¸æ»¡è¶³æ¡ä»¶")
            time.sleep(60)

        